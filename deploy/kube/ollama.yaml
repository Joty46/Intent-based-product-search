---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: "backend"
  name: "backend-ollama"
spec:
  selector:
    matchLabels:
      app: "ollama"
  template:
    metadata:
      labels:
        app: "ollama"
    spec:
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: ollama-model-persistent-volume-claim
        - name: file-storage
          persistentVolumeClaim:
            claimName: ollama-file-persistent-volume-claim
        - name: script
          configMap:
            name: ollama-script
            defaultMode: 0755
      containers:
        - name: ollama
          image: ollama/ollama:latest
          volumeMounts:
            - name: model-storage
              mountPath: /root/.ollama
              subPath: ollama-model
            - name: file-storage
              mountPath: /model_files
              subPath: ollama-file
            - name: script
              mountPath: /model_files/run_ollama.sh
              subPath: run_ollama.sh
          command: ["/bin/sh", "/model_files/run_ollama.sh"]
          ports:
            - containerPort: 11434
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-model-persistent-volume-claim
  namespace: resource
spec:
  accessModes: 
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-file-persistent-volume-claim
  namespace: resource
spec:
  accessModes: 
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: v1
kind: Service
metadata:
  namespace: "backend"
  name: "ollama-service"
spec:
  selector:
    app: "ollama"
  ports:
    - port: 11434
      targetPort: 11434

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-script
data:
  run_ollama.sh: |
    #!/bin/bash

    echo "Starting Ollama server..."
    ollama serve &
    # Wait for Ollama HTTP API to respond
    echo "Waiting for Ollama server to be active..."
    sleep 1

    # Pull the model if it isnâ€™t already present
    if ! ollama list | grep -q 'qwen2.5:3b-instruct'; then
      echo "Pulling qwen2.5:3b-instruct..."
      ollama pull qwen2.5:3b-instruct
    fi

    if ! ollama list | grep -q 'nomic-embed-text'; then
      echo "Pulling nomic-embed-text..."
      ollama pull nomic-embed-text
    fi

    # Finally, run the model in the foreground to keep the container alive
    echo "Running qwen2.5:3b-instruct..."
    ollama run qwen2.5:3b-instruct 

